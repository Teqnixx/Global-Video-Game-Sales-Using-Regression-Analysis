# -*- coding: utf-8 -*-
"""Global Video Game Sales Using Regression Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K0Ct7JCYpp0xElgyXwfpQKceo09GSCz1
"""

import numpy as np
import pandas as pd

df = pd.read_csv('GlobalVideoGameSales.csv')

df.head(10)

df.dtypes

df.isna().sum()

import matplotlib.pyplot as plt
import seaborn as sns

numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns

for x in numeric_columns:
  data = df[x].dropna()

  plt.figure(figsize=(12, 4))
  plt.subplot(1, 2, 1)
  sns.histplot(data, kde=True)
  plt.title(f'Histogram of ' + x)

  sk = data.skew()

  print(f"Column:", x)
  print(f"Skewness:", sk)
  print(f"Absolute Skewness: ", abs(sk))

  if abs(sk) < 0.5:
      print("Distribution is approximately symmetric (normal)")
  elif sk > 0.5:
      print("Distribution is right-skewed")
  elif sk < -0.5:
      print("Distribution is left-skewed")

  plt.tight_layout()
  plt.show()
  print()

"""##Data Pre-Processing
###1. Drop Duplicates
"""

print(df.shape)
df = df.drop_duplicates()
print(df.shape)

"""###2. Handle Missing Values"""

fillWithNA = ["Name", "Year_of_Release", "Genre", "Publisher", "Developer", "Rating"]
fillWithMedian = ["Critic_Score", "Critic_Count", "User_Score", "User_Count"]

for i in df.columns:
  if i in fillWithNA:
    df[i] = df[i].fillna("NA")

df.loc[df["User_Score"] == "tbd", "User_Score"] = "NaN"
df["User_Score"] = df["User_Score"].astype(float)

for i in df.columns:
  if i in fillWithMedian:
    df[i] = df[i].fillna(df[i].median())

df["Year_of_Release"] = df["Year_of_Release"].astype("str")

"""###3. Handle Extreme Values"""

def getOutliers(dataset,key):
    q1 = dataset[key].quantile(0.25);
    q3 = dataset[key].quantile(0.75);
    iqr = q3 - q1;
    max_ = q3 + (iqr * 1.5);
    min_ = q1 - (iqr * 1.5);
    dataset.loc[dataset[key] < min_,key] = min_;
    dataset.loc[dataset[key] > max_,key] = max_;

import matplotlib.pyplot as plt

plt.rcParams["figure.figsize"] = [7.50, 3.50]
plt.rcParams["figure.autolayout"] = True

ax = df[['Global_Sales', 'NA_Sales']].plot(kind='box', title='boxplot')

plt.show()

for i in df.columns:
  if df[i].dtypes != "object":
    getOutliers(df, i);

ax = df[['Global_Sales', 'NA_Sales']].plot(kind='box', title='boxplot')

plt.show()

"""###4. Remove Duplicate and Unnecessary Columns"""

df = df.drop(["Name"], axis=1)

"""###5. One Hot Encoding / Label Encoding"""

from sklearn.preprocessing import LabelEncoder

newdf = df
for i in newdf.columns:
  if df[i].dtypes == "object":
    encoder = LabelEncoder()
    newdf[i] = encoder.fit_transform(newdf[i])

import seaborn as sns
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(15, 10))
sns.heatmap(newdf.corr(), annot=True, linewidths=.8, ax=ax)

newdf = newdf.drop(["Critic_Score", "Critic_Count", "User_Score", "User_Count"], axis=1)

fig, ax = plt.subplots(figsize=(15, 10))
sns.heatmap(newdf.corr(), annot=True, linewidths=.8, ax=ax)

corrdf = newdf.corr()
corrdf

dff = corrdf.unstack().reset_index();
dff.loc[(abs(dff[0]) >= .75) & (dff["level_0"] != dff["level_1"])]

df = df.drop(["EU_Sales", "Other_Sales"], axis=1)

"""##Since the target variable doesn't correlate with other columns if the outliers is removed, we decided to remove the outliers on the target variable as the result of our correlation analysis."""

corrdf = df.corr()
dff = corrdf.unstack().reset_index();
dff.loc[(abs(dff[0]) >= .75) & (dff["level_0"] != dff["level_1"])]

"""##Data Normalization"""

from sklearn.preprocessing import StandardScaler

for i in newdf.columns:
    if i != "Global_Sales":
        scaler = StandardScaler();
        newdf[i] = scaler.fit_transform(newdf[i].values.reshape(-1, 1));

newdf

"""##Split Dataset"""

X = newdf.drop(["Global_Sales"], axis=1);
y = newdf["Global_Sales"];

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size=0.75)

"""##Fit dataset into a model"""

from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error

def evaluate_model(model):
    print("R2 score (TRAINING): ", r2_score(y_train,model.predict(X_train)));
    print("R2 score (TEST): ", r2_score(y_test,model.predict(X_test)));
    print("MSE: ", mean_squared_error(y_test,model.predict(X_test)));
    print("MAE: ", mean_absolute_error(y_test,model.predict(X_test)));
    print("RMSE: ", np.sqrt(mean_squared_error(y_test,model.predict(X_test))));

    from matplotlib import pyplot as plt

    plt.scatter(y_test,model.predict(X_test))
    plt.plot(model.predict(X_test),model.predict(X_test),color="red")

"""##Evaluate Models"""

from sklearn.linear_model import LinearRegression

lr = LinearRegression();
lr.fit(X_train,y_train);
evaluate_model(lr)

from sklearn.linear_model import Ridge

ridge = Ridge();
ridge.fit(X_train,y_train);
evaluate_model(ridge)

from sklearn.linear_model import Lasso

lasso = Lasso();
lasso.fit(X_train,y_train);
evaluate_model(lasso)

from sklearn.tree import DecisionTreeRegressor

dt = DecisionTreeRegressor(criterion="squared_error", max_depth=5)
dt.fit(X_train,y_train)
evaluate_model(dt)

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(criterion="squared_error", n_estimators=100, max_depth=10)
rf.fit(X_train, y_train)
evaluate_model(rf)

"""#The best model for this dataset is Random Forest Regressor with criterion of squared error, 100 n_estimators and max depth of 10 with R2 score of 97%.

###Note: Outlier of target variable is removed as a result in correlation analysis
"""

